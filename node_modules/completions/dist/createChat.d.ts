import { type CompletionsOptions, type Message, Choice } from "./createCompletions";
import { type JSONSchema, type FromSchema } from "json-schema-to-ts";
export type ShallowJsonObject = {
    [k: string]: string;
};
type Expectation = {
    examples: ShallowJsonObject[];
    schema: JSONSchema;
};
type MessageOptions = Partial<Omit<CompletionsOptions, "messages" | "n" | "functions">> & {
    expect?: Expectation;
};
type StructuredChoice<T> = Omit<Choice, "content"> & {
    content: T;
};
interface SendMessage {
    <T extends MessageOptions>(prompt: string, messageOptions: T): Promise<StructuredChoice<FromSchema<T["expect"]["schema"]>>>;
    (prompt: string, messageOptions?: MessageOptions): Promise<Choice>;
}
export type Chat = {
    addMessage: (message: Message) => void;
    getMessages: () => Message[];
    sendMessage: SendMessage;
};
/**
 * @property apiKey - OpenAI API key.
 * @property frequencyPenalty - Number between -2.0 and 2.0. Positive values penalize new
 *    tokens based on their existing frequency in the text so far, decreasing the model's
 *    likelihood to repeat the same line verbatim.
 * @property logitBias - Number between -2.0 and 2.0. Positive values penalize new tokens
 *    based on their existing frequency in the text so far, decreasing the model's likelihood to
 *    repeat the same line verbatim.
 * @property maxTokens – The maximum number of tokens to generate in the chat completion.
 *    The total length of input tokens and generated tokens is limited by the model's context length.
 * @property model - ID of the model to use. See the model endpoint compatibility table for
 *    details on which models work with the Chat API.
 * @property functionCall - Controls how the model responds to function calls.
 *    "none" means the model does not call a function, and responds to the end-user.
 *    "auto" means the model can pick between an end-user or calling a function.
 *    Specifying a particular function via {"name":\ "my_function"} forces the model to call that function.
 *    "none" is the default when no functions are present.
 *    "auto" is the default if functions are present.
 * @property functions - A list of functions the model may generate JSON inputs for.
 * @property n - How many chat completion choices to generate for each input message.
 * @property presencePenalty - Number between -2.0 and 2.0. Positive values penalize new
 *    tokens based on whether they appear in the text so far, increasing the model's
 *    likelihood to talk about new topics.
 * @property stop - Up to 4 sequences where the API will stop generating further tokens.
 * @property temperature - What sampling temperature to use, between 0 and 2. Higher values
 *    like 0.8 will make the output more random, while lower values like 0.2 will make it
 *    more focused and deterministic.
 *    We generally recommend altering this or top_p but not both.
 * @property topP - An alternative to sampling with temperature, called nucleus sampling,
 *    where the model considers the results of the tokens with top_p probability mass.
 *    So 0.1 means only the tokens comprising the top 10% probability mass are considered.
 *    We generally recommend altering this or temperature but not both.
 * @property user - A unique identifier representing your end-user, which can help OpenAI
 *    to monitor and detect abuse.
 */
export declare const createChat: (options: Omit<CompletionsOptions, "messages" | "n" | "onUpdate">) => Chat;
export {};
